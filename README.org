* Ttl

** What
   
*** Good
The beautiful thing about org-mode is the customizability. 
 - org-agenda
 - org-habit
 - org-capture
 - org-journal
 - org-contacts
 - timetracking
 - tables / spreadsheet
 - org-pomodoro
 - org-wiki
 - org-brain
 - org-present?
 - calendar invites
 - export to html/etc.
 - fold / unfold
 - scheduling with other people
 
Acts as an agenda, outliner, wiki, blog, etc. It's secure because single-user. Easy to backup, etc. The best of all worlds if there is only a single device.

*** Bad
1) Cannot easily capture web data, pictures, video, clips, etc.
2) Not simple/easy to access over mobile. Though there are mobileorg and orgzly. 
3) Collaboration (though will need to think about security model here)
   - Whether through publishing blog, document
   - Allowing sharing / delegation of tasks
   - Comments 

*** How
- Capture
- Schedule the time to do it.

- Goals / things / tasks that want to accomplish
- Countdown the number of days to accomplish 
- Get reminded
- Encouragement from others who have the same goal
- Markdown?
- Sharing / Delegation
- Publish
- Reminder / categorization - groceries, etc based on location
  - Notifications 
    - https://github.com/realtime-framework/WebPushNotifications/blob/master/index.js
    - https://pushcrew.com/pricing/
    - https://gauntface.github.io/simple-push-demo/
    - https://github.com/GoogleChrome/samples/tree/gh-pages/push-messaging-and-notifications
    - https://github.com/web-push-libs/web-push-php
    - https://elixirforum.com/t/what-is-the-simplest-way-to-send-notification-from-elixir-app-to-android-device/6264/4

Object/Item/Task (this is the fundamental unit - could be part of goal or habit)
  - UserId
  - ObjectId
  - GoalId maybe null
  - HabitId maybe null
  - ReferenceId (jira, email, picture, note, outlook, integrations, comments)
  - Attachments
  - Properties (likable, commentable, private, etc)
  - Path - in case there are many subtasks
  - Blob
  - Minimum time needed
  - Time spent
  - Time left
  - State
  - Times deferred

This becomes an adventure in either parsing the document and storing everything as one item. Or having to parse the document itself.

Goals
  - Habit (streak based)
  - Time limit
    - Weekly
    - 40 days (short)
    - 6 months (medium)
    - 1 year (long)
  - Success criteria?
  - Could be suggested
  - Breakdown Items + Time
  - Weekly review
  - Reward
  - Start Date
  - End Date

Interaction
  - Comments
  - Reaction (time, parent, author)

Prioritizer
  - bin-packing based on min-time, deadline, priority
  - Every x-y days
  - Schedule
  - Deadline
  - Priority
  - Prior scheduling that worked and was successful
  - Saves the event and the suggestion

Groups
  - set of users
  - topics
  - interactions

Interaction
  - ObjectId
  - UserId
  - Comments
  - Reaction (time, parent, author)

Journal
  - day
  - rating
  - frequency

Open Times
  - UserId
  - Calendar
  - Scheduling
  - Tags associated with times
  - Enum(Tagged - will schedule with tag, Open)

  - Calendar implementation:
    - user_id + template
    - template:
      - { day, date, time, type={"base", "override"}, tag={"work", "sleep", etc} }
      - {[1-5], nil, [9-17], "base", "work"}
      - {[1-5], nil, [8-830], "base", "commute"}
      - {[1-5], nil, [1630-1730], "base", "commute"}
      - {nil, 2017-07-04, nil, type="override", tag={"offday"}


Tags
  - project (assoc with work)
  - podcast (assoc with any)
  - gardening (assoc with home)
  - woodworking (assoc with home)
  - health (assoc with any)
  - meditation (assoc with home)
  - reading (assoc with any)
  - writing (assoc with any)
  - hiking (assoc with offday)

Context
  - bus
  - home
  - work
  - in transport
  - offday

State:
  - stuck
  - delay
  - 5min
  - done
  - open
  - started

*** Others
https://checkvist.com/checklists/627469-name-your-project-and-press-enter
https://workflowy.com/ -- the most mature app
https://www.moo.do -- really nice Google Apps integration
https://checkvist.com/ -- extremely feature rich
https://github.com/novoid/Memacs
https://turtlapp.com/
http://www.makeuseof.com/tag/lightweight-onenote-evernote-alternatives/
https://github.com/scripting/concord
Personal knowledge base:
https://news.ycombinator.com/item?id=8270759
    
  
*** Org-mode for editors
https://atom.io/packages/organized - https://github.com/MattFlower/organized
https://github.com/danielmagnussons/orgmode
Msg thread - https://lists.gnu.org/archive/html/emacs-orgmode/2014-10/msg00212.html
https://github.com/ajaxorg/ace/wiki/Creating-or-Extending-an-Edit-Mode


** Workflow
*** Sync 
This will send the entire file over and at high load and high users cause issues. Will need a emacs client to update properties. 

- Have things locally.
- Initial sync
- Server parses and adds uuid's
  - Stores in database so can do scheduling, sharing, making lists for other people
  - Re-exports with additional property information
- No conflict resolution between local/remote copies.
  Should have a time-vector on each element though.
  Pull down any changes from server and use them. Sync to server overwrites the uploads/changes.
- This workflow requires an api:
  - Import/export whole documents (this can be used by emacs and/or the web editor)

*** Collaboration
A simple use case is someone making a list for someone else to do. Grocery list, etc. 
When is something allowed to be delegated? What are the viewing permissions? Are there spaces with permissions/groups instead? 
Commenting then can be on objects as a generic comment service. However, how would the comments be fetched? They would have to have the same permissions as the parent. Don't want the comment service to have to call back to the parent service for permissions. The other option is replicating / synchronizing the permissions over the the comment system. Another option is only have trusted systems call the comment system and never have it public.    
*** Capturing
Major issue is capturing images, bookmarks, webpages. 

https://github.com/alphapapa/org-protocol-capture-html has some bookmarklets. 

** DONE MVP1
   CLOSED: [2017-09-01 Fri 17:23]
*** DONE Thoughts on parsing and db schema:
    CLOSED: [2017-08-24 Thu 20:59]
  - Can store the entire file and then the tags/headers in separate tables for scheduling
    - If the agenda is modified on web, item state to done, the file needs to be re-written also. Need to keep both in sync.
  - Represent the entire file as an AST and then store it
    - AST table (how to deal with deleted fragments?)
      - doc | [element_paths, uuid1, uuid2, uuid3]  -> export selects and joins all the elements in the path?
        object | [ uuid, uid, gid, content ]
      - how to represent header levels, planning, property, then all the stuff under the header?
      - [header(level=1, uuid=x, planning, property), section(uuid=x, content), header(level=2, uuid=x, planning, property)
      - how to represent a list with tags and without tags?
        - answer: don't. adds complications for now and no point
      - how to collaborate? have another field with users that have access/edit to it? treat each object as a gco
      - associated with a single object - header, planning, properties, section, timelog
      - timelog should have the logbook and the state transitions? or separate tables?
  #+NAME: DBSCHEMA
  - Database schema

     #mix phx.gen.html Things Document documents user_id:references:accounts_users name:string objects:array:uuid:references_things_objects
     mix phx.gen.html Things Document documents user_id:references:accounts_users name:string objects:array:uuid
   
     mix phx.gen.html Things Object objects document_id:references:things_documents path:array:uuid level:integer title:text content:text blob:binary closed:utc_datetime scheduled:utc_datetime deadline:utc_datetime state:string priority:string version:integer defer_count:integer min_time_needed:integer time_spent:integer time_left:integer permissions:integer

     mix phx.gen.html Things Tag tags user_id:references:accounts_users tag:string

     Maybe add a type column to these and call it a day?
     mix phx.gen.html Things Property properties object_id:references:things_objects key:string value:string
     mix phx.gen.html Things Timelog timelogs object_id:references:things_objects key:string value:string
 
     create many to many table things_objects_tags
     create many to many table things_objects_properties

     A document is an array of objects, a header has properties, tags, logbook associated with it, and section. Could also make it more of a tree-like structure, but we're not trying to edit the document, we're trying to get the main parts to do some scheduling and be able to regenerate it.

     An annoying problem is that we want to be able to refresh a document and also reorder or remove any objects. Possible to have a table of UUID's in the document to guarantee order. Another possibility is a many table with reference to document. If objects moved up and down, we would have to update every object. Same for insertion for new objects in the middle of a file. With an array of uuid's, need to just update the array and take care of fragments later. 


      


    - old notes header is essentially an object, but then what is a section? section will be the content of a header
      what about logbook?

    if we want to get a subtree, we also need to put in the path correctly for the subtree
    or can generate subtree - from the doc uuid's

  - how to get a particular ordered subtree of a document?
    we should have a document with a set of uuid's in an order. we need to select up to the last uuid where level = the level we're at
    CREATE EXTENSION pgcrypto; 
    CREATE TABLE contacts(  id UUID PRIMARY KEY DEFAULT gen_random_uuid(), name TEXT, email TEXT);

    snw=# with x (id_list) as ( values (array['bb8f6873-b2f7-4109-a35f-308930e1d57c', '3ccd8a17-0fac-484c-83ab-feacf88c1812', '2eb23b9f-eb9d-40d0-a46a-24c50aa73173'])) select c.* from contacts c, x where cast(id as text) = any (x.id_list) order by array_position(x.id_list, cast( c.id as text));
    id                  |      name       | email 
    --------------------------------------+-----------------+-------
    bb8f6873-b2f7-4109-a35f-308930e1d57c | Geoff Franks    | geoff
    3ccd8a17-0fac-484c-83ab-feacf88c1812 | Dr Nic Williams | drnic
    2eb23b9f-eb9d-40d0-a46a-24c50aa73173 | Jamie Van Dyke  | jamie

  - ttl_dev=# with x (id_list) as ( values (array['ffce2bd3-a4c4-4ea9-98df-e4ebde4452ee','75c3758c-0af9-4f15-a75a-c66a2fbe0299','f3c99da2-f9ef-45d8
-a59d-c2b606be52d6'])) select o.id, o.title from things_objects o, x where cast(id as text) = any (x.id_list) order by array_position(x.id_list
, cast( o.id as text));  
  - with x (id_list) as (select objects from things_documents) select o.id, o.title from things_objects o, x where id = any (x.id_list) order by array_position(x.id_list, o.id );


*** DONE Thoughts on UI Interface
    CLOSED: [2017-08-24 Thu 20:59]

- If this is for power-users, it has to be able to integrate with org-mode. To import/export out of text mode, there needs to be a UUID for each element.

- If tasks can be shared / commented / etc. each item will have to be independent. Also, inheritance is important here, probably just from the top-level. 
- How would conflict resolution be handled? Not sure how to merge diffs between different versions except to have a time-clock.

- https://org-web.org/ - is a rudimentary org-web parser
- http://appsonthemove.com/blog/2017/05/25/borg/
- http://www.orgzly.com/help#org7fa55de
- https://www.reddit.com/r/emacs/comments/6r5htr/capturing_short_notes_on_the_go_then_sync_to_org/

  
*** DONE import/export to api. 
    CLOSED: [2017-08-19 Sat 22:11]
**** DONE write a parser (not going to do in elisp since may want non-emacs users)
     CLOSED: [2017-08-15 Tue 18:27]
**** DONE create db schema
CLOSED: [2017-08-13 Sun 15:01]
[[DBSCHEMA]]
**** DONE consolidate sections
CLOSED: [2017-08-13 Sun 15:00]
:LOGBOOK:
CLOCK: [2017-08-13 Sun 12:14]--[2017-08-13 Sun 12:39] =>  0:25
CLOCK: [2017-08-13 Sun 11:44]--[2017-08-13 Sun 12:09] =>  0:25
CLOCK: [2017-08-13 Sun 11:14]--[2017-08-13 Sun 11:39] =>  0:25
CLOCK: [2017-08-13 Sun 07:18]--[2017-08-13 Sun 07:44] =>  0:26
CLOCK: [2017-08-13 Sun 06:46]--[2017-08-13 Sun 07:12] =>  0:26
:END:
**** DONE Need to fix the planning parser to grab the dates
CLOSED: [2017-08-13 Sun 15:00]
:LOGBOOK:
CLOCK: [2017-08-13 Sun 12:48]--[2017-08-13 Sun 13:13] =>  0:25
:END:
**** DONE consolidate header (property, planning)
CLOSED: [2017-08-13 Sun 15:00]
**** DONE implement db schema
CLOSED: [2017-08-14 Mon 16:44]
:LOGBOOK:
CLOCK: [2017-08-14 Mon 16:14]--[2017-08-14 Mon 16:39] =>  0:25
CLOCK: [2017-08-14 Mon 15:44]--[2017-08-14 Mon 16:09] =>  0:25
CLOCK: [2017-08-14 Mon 15:09]--[2017-08-14 Mon 15:34] =>  0:25
:END:
**** DONE write to database
     CLOSED: [2017-08-15 Tue 21:48]
:LOGBOOK:
CLOCK: [2017-08-15 Tue 11:13]--[2017-08-15 Tue 11:38] =>  0:25
CLOCK: [2017-08-15 Tue 10:44]--[2017-08-15 Tue 11:09] =>  0:25
CLOCK: [2017-08-15 Tue 10:14]--[2017-08-15 Tue 10:39] =>  0:25
CLOCK: [2017-08-15 Tue 09:44]--[2017-08-15 Tue 10:09] =>  0:25
CLOCK: [2017-08-15 Tue 09:11]--[2017-08-15 Tue 09:36] =>  0:25
CLOCK: [2017-08-14 Mon 19:26]--[2017-08-14 Mon 19:46] =>  0:20
CLOCK: [2017-08-14 Mon 18:57]--[2017-08-14 Mon 19:26] =>  0:29
CLOCK: [2017-08-14 Mon 18:31]--[2017-08-14 Mon 18:56] =>  0:25
CLOCK: [2017-08-14 Mon 18:05]--[2017-08-14 Mon 18:30] =>  0:25
CLOCK: [2017-08-14 Mon 17:13]--[2017-08-14 Mon 17:38] =>  0:25
CLOCK: [2017-08-14 Mon 16:44]--[2017-08-14 Mon 17:10] =>  0:26
:END:
The dates need to be parsed to put into the db
Turns out they are of many variable formats, need to use regexp
The scheduler needs to have a few additional fields in db

After mucking around quite a bit with dates, regret not using a lexer. 
Turns out Ecto casting from naive datetime to datetime was the issue. 

Takes a document, parses it, and can insert all the objects into the database

***** DONE Make it into a function 
      CLOSED: [2017-08-15 Tue 20:06]
      :LOGBOOK:
      CLOCK: [2017-08-15 Tue 19:30]--[2017-08-15 Tue 20:00] =>  0:30
      :END:
***** DONE Make a solid decision on what to do on failed update / version conflict
      CLOSED: [2017-08-15 Tue 21:40]
      :LOGBOOK:
      CLOCK: [2017-08-15 Tue 21:00]--[2017-08-15 Tue 21:38] =>  0:38
      CLOCK: [2017-08-15 Tue 20:13]--[2017-08-15 Tue 20:38] =>  0:25
      :END:
***** DONE Parse and cast the dates
      CLOSED: [2017-08-15 Tue 18:19]
      http://orgmode.org/manual/Timestamps.html#Timestamps

      :LOGBOOK:
      CLOCK: [2017-08-15 Tue 16:55]--[2017-08-15 Tue 18:14] =>  1:19
      CLOCK: [2017-08-15 Tue 16:15]--[2017-08-15 Tue 16:51] =>  0:36
      CLOCK: [2017-08-15 Tue 15:30]--[2017-08-15 Tue 16:11] =>  0:41
      CLOCK: [2017-08-15 Tue 15:04]--[2017-08-15 Tue 15:29] =>  0:25
      CLOCK: [2017-08-15 Tue 13:25]--[2017-08-15 Tue 13:50] =>  0:25
      CLOCK: [2017-08-15 Tue 12:43]--[2017-08-15 Tue 13:25] =>  0:42
      CLOCK: [2017-08-15 Tue 12:16]--[2017-08-15 Tue 12:41] =>  0:25
      CLOCK: [2017-08-15 Tue 11:43]--[2017-08-15 Tue 12:08] =>  0:25
      :END:
***** DONE Or maybe need to do versioning right now?
      CLOSED: [2017-08-15 Tue 18:22]
**** DONE Need to parse the file metadata
     CLOSED: [2017-08-15 Tue 23:18]
      CLOCK: [2017-08-15 Tue 21:45]--[2017-08-15 Tue 23:20] =>  1:40
written to the database as jsonb
http://ehneilsen.net/notebook/orgExamples/org-examples.html#sec-2

**** DONE regenerate file from database - with uuid's
     CLOSED: [2017-08-16 Wed 19:18]
     :LOGBOOK:
     CLOCK: [2017-08-16 Wed 18:45]--[2017-08-16 Wed 19:20] =>  0:35
     CLOCK: [2017-08-16 Wed 18:15]--[2017-08-16 Wed 18:40] =>  0:25
     CLOCK: [2017-08-16 Wed 17:40]--[2017-08-16 Wed 18:15] =>  0:35
     CLOCK: [2017-08-16 Wed 17:10]--[2017-08-16 Wed 17:38] =>  0:28
     CLOCK: [2017-08-16 Wed 16:40]--[2017-08-16 Wed 17:08] =>  0:28
     CLOCK: [2017-08-16 Wed 16:11]--[2017-08-16 Wed 16:36] =>  0:25
     CLOCK: [2017-08-16 Wed 12:03]--[2017-08-16 Wed 12:28] =>  0:25
     CLOCK: [2017-08-16 Wed 11:28]--[2017-08-16 Wed 11:53] =>  0:25
     CLOCK: [2017-08-16 Wed 11:05]--[2017-08-16 Wed 11:20] =>  0:15
     CLOCK: [2017-08-16 Wed 10:36]--[2017-08-16 Wed 11:01] =>  0:25
     CLOCK: [2017-08-16 Wed 08:45]--[2017-08-16 Wed 09:00] =>  0:15
     CLOCK: [2017-08-16 Wed 07:25]--[2017-08-16 Wed 07:43] =>  0:18
     :END:
    - The file uuid goes in the metadata at the top
    - The object metadata goes into properties
    - ttl_dev=# with x (id_list) as (select objects from things_documents) select o.id, o.title from things_objects o, x where id = any (x.id_list) order by array_position(x.id_list, o.id  );
    - ttl_dev=# with x (id_list) as (select objects from things_documents) select o.id, o.title from things_objects o, d.metadata from things_documents d,  x where id = any (x.id_list) order by array_position(x.id_list, o.id  );
    - 
***** DONE adjust the levels for files with no bullets to 0 - this will fix for files with no headline
      CLOSED: [2017-08-16 Wed 07:22]
***** DONE bug with closed planning data not being parsed
      CLOSED: [2017-08-16 Wed 07:43]

**** DONE Bugs - adil file has extra newline at start
     CLOSED: [2017-08-16 Wed 19:18]
**** DONE Properties - parse, store, and write
     CLOSED: [2017-08-16 Wed 22:59]
     :LOGBOOK:
     CLOCK: [2017-08-16 Wed 21:10]--[2017-08-16 Wed 22:57] =>  1:47
     :END: 
**** DONE Add file uuid into regenerate
     CLOSED: [2017-08-16 Wed 23:39]
**** DONE Add version and id into regenerated object 
     CLOSED: [2017-08-17 Thu 00:03]
     :LOGBOOK:
     CLOCK: [2017-08-16 Wed 23:39]--[2017-08-17 Thu 00:03] =>  0:24
     :END:
**** DONE Read file uuid from file to pull objects
     CLOSED: [2017-08-17 Thu 11:38]
     :LOGBOOK:
     CLOCK: [2017-08-17 Thu 10:00]--[2017-08-17 Thu 10:57] =>  0:57
     CLOCK: [2017-08-17 Thu 08:55]--[2017-08-17 Thu 09:20] =>  0:25
     CLOCK: [2017-08-17 Thu 08:05]--[2017-08-17 Thu 08:50] =>  0:45
     :END:
**** DONE Start of file exception
     CLOSED: [2017-08-17 Thu 11:38]
     :LOGBOOK:
     CLOCK: [2017-08-17 Thu 11:04]--[2017-08-17 Thu 11:29] =>  0:25
     :END:
**** DONE Ensure data gets updated on change
     CLOSED: [2017-08-17 Thu 13:05]
     :LOGBOOK:
     CLOCK: [2017-08-17 Thu 12:58]--[2017-08-17 Thu 13:06] =>  0:08
     :END:
      Org-mode won't ever update the version locally. If remote_version > local_version => conflict.
      Conflict resolution will require parsing things locally and replacing them
      Tests:
       - An old object (all db counts same)
       - A new object gets added (db count + 1)
       - the new object gets modified (db count same)

**** DONE make an interval data structure for schedule parsing and bin-packing calendar with contexts / tags
     CLOSED: [2017-08-17 Thu 17:53]
     :LOGBOOK:
     CLOCK: [2017-08-19 Sat 11:12]--[2017-08-19 Sat 11:37] =>  0:25
     CLOCK: [2017-08-19 Sat 10:37]--[2017-08-19 Sat 11:02] =>  0:25
     CLOCK: [2017-08-17 Thu 13:42]--[2017-08-17 Thu 14:07] =>  0:25
     :END:
Time can be tagged with multiple contexts. So an interval of time can be tagged work, etc. 
org-mode also has intervals which need to be stored for scheduled time and repetition. 
      CLOSED: [2016-06-02 Thu 21:22] SCHEDULED: <2016-06-08 Wed 09:00+x>--<2016-06-02 9:00-17:00> DEADLINE: <bla>

The agenda display is not important in this case, the api output will need to be generated anyway. 
Easiest format for a single day would be {start_time, interval_size}. This allows for packing things into whatever interval_size available. 
The end times for org-document generation will need to be calculated.  
For the issue of "scheduled" over multiple days, it will need to become a list. [{start_time1, interval1}, {start_time2, interval2}] and repeat_interval
In the db - not sure how to store this? another jsonb?
Use two arrays - one for start time, the other for interval. 
Other option - just have a planning table. 
select * from objects o, planning p where o.id = p.object_id and p.scheduled > $p1 and p.scheduled < $p2 

UPDATE - just going with start_time, start_interval, date_range

     mix phx.gen.html Things Object objects document_id:references:things_documents path:array:uuid level:integer title:text content:text blob:binary state:string priority:string version:integer defer_count:integer min_time_needed:integer time_spent:integer time_left:integer permissions:integer
     mix phx.gen.html Things Planning planning object_id:references:things_objects repeat_interval:string closed:utc_datetime scheduled:utc_datetime scheduled_date_range:integer scheduled_time_interval:integer deadline:utc_datetime 
     mix phx.gen.html Things Property properties object_id:references:things_objects key:string value:string
     mix phx.gen.html Things Timelog timelogs object_id:references:things_objects key:string value:string

Workflow:
Server sees free time. Sorts by priority + tag matching on time slot + longest first? Need to optimize algorithm based on actual results. 
Yes/no -> deferred -> repeat.  
Or set out a full schedule at night. 
Data structure is fine. 
However, need ability to defer scheduled tasks and logging.


**** DONE Parse the schedule properly into the interval structure
     CLOSED: [2017-08-19 Sat 17:53]
     :LOGBOOK:
     CLOCK: [2017-08-17 Thu 18:55]--[2017-08-17 Thu 19:20] =>  0:25
     CLOCK: [2017-08-17 Thu 18:25]--[2017-08-17 Thu 18:50] =>  0:25
     CLOCK: [2017-08-17 Thu 17:53]--[2017-08-17 Thu 18:20] =>  0:27
     :END:
Current state - schedule start_date parsing has been done. End_date, duration, repeat_interval remain. Data structure needs put into schema and then populated.

 
**** DONE Bugs parsing schedule time interval + repeat - is this done later?
     CLOSED: [2017-08-19 Sat 17:53]
**** DONE Bugs regeneration date format should be the same
     CLOSED: [2017-08-19 Sat 22:07]
**** DONE Parse or skip logbook 
     CLOSED: [2017-08-19 Sat 22:07]
      - probably a separate table. Same as tags. Did insert_all with ecto which may complicate this implementation
*** DONE Cleanup
    CLOSED: [2017-09-01 Fri 17:51]
**** DONE Refactor parse 
     CLOSED: [2017-09-01 Fri 17:23] SCHEDULED: <2017-08-24 Thu 18:00-20:00> DEADLINE: <2017-08-24 Thu 21:00>
     :LOGBOOK:
     CLOCK: [2017-08-24 Thu 17:30]--[2017-08-24 Thu 20:49] =>  3:19
     :END:
**** use ecto.multi or control the dual commit transaction
*** DONE POC - display UI for agenda
   :LOGBOOK:
   CLOCK: [2017-09-01 Fri 00:00]--[2017-09-01 Fri 23:59] => 23:59
   :END:
    CLOSED: [2017-09-01 Fri 10:17]
Not sure which is more important to do first. Capturing, offline support, notifications, logging activities. Notifications are fairly straightforward. Capture and offline support will need to come by building out a service worker for a progressive web app (pwa) . Looks like a lot more functionality will end up moving into the client. 

The server will contain the documents and the items stored in kinto db. 
The client will sync proxy auth through the server to make things consistent.
Client downloads entire documents and has all the data now.
  - capture
  - display the agenda
    - schedule / reschedule
    - mark as done
    - logging time / journal
    - import/export back to org-mode
  - most important tasks

  - https://www.realsimple.com/home-organizing/organizing/bullet-journal
  - http://help.bulletjournal.com/category/5-bullet-journaling-101
  - https://news.ycombinator.com/item?id=11856987
  - http://talk.dynalist.io/t/switching-to-dynalist-from-workflowy-and-todoist/475
  - memex - what did you do on what day
  - reminder to journal
**** PWA

Looks like an app-shell needs to be implemented and workboxjs seems to be the newest gold standard. 

Workflow:

Desires:
- communication over websocket
- little to no api if not required
- works offline

- Agenda shell
  - This is a rendered html page without data
  - Has some authorization info locally? 
  - Fetches from local to display immediately 
  - Fetches from remote to sync
    - update cache
    - and then redisplay on update

https://elixirforum.com/t/tips-for-building-resilient-frontend-apps-which-use-phoenix-channels-for-backend-communication/6216/15
https://www.viget.com/articles/phoenix-and-react-a-killer-combo
https://unpoly.com/
https://github.com/hyperapp/hyperapp


https://developers.google.com/web/fundamentals/instant-and-offline/offline-cookbook/

https://hnpwa.com/

https://github.com/Kinto/kinto/wiki/Brainstorm-rationale

Should probably take a PWA style working model and modify it:
https://github.com/SimonDEvans/notes

https://serviceworke.rs/
https://workboxjs.org/
https://nolanlawson.github.io/fronteers-2016/#/51
https://github.com/turbolinks/turbolinks
https://github.com/GoogleChrome/workbox
https://github.com/GoogleChrome/voice-memos
https://github.com/jakearchibald/trained-to-thrill/

**** Drab
Spent a few hours off and on investigating Drab. Server-side dom modifications is nice, but not sure how to deal with offline. Will have to build custom sync mechanism. 
**** Clojurescript
Want to use. Community is small compared to others though.

We'd have to make the externs for kinto. 
https://github.com/cljsjs

However, before doing anything with clojurescript get something working with javascript.
https://github.com/andreloureiro/andrel.me/blob/master/_posts/2016-01-09-a-basic-service-worker-implementation.md

Figwheel is pretty awesome. 

https://github.com/gadfly361/cljs-todomvc
https://github.com/tel/oak 

**** Kinto
     :LOGBOOK:
     CLOCK: [2017-08-30 Wed 19:00]--[2017-08-30 Wed 23:48] =>  4:48
     :END:

Kinto philosophy and architecture fits better. 

Comparison with Pouchdb and others:
http://www.servicedenuages.fr/en/generic-storage-ecosystem#storage-specs
http://docs.kinto-storage.org/en/stable/faq.html#comparison

Only other choice for offline storage was pouchdb, however, the cons are:
  - This will then require a db per user and the sharing of objects isn't possible easily anymore.
  - The permissioning requires a gateway from cloudant or etc. and going down that rabbithole. 

Close to the same data schema is possible with documents moving to a "collection" and all the objects into the "records". Groups, permissions, sharing out of the box. Possible performance issues on large datasets, but very unlikely. Fairly robust permissions for grouping:
http://docs.kinto-storage.org/en/stable/tutorials/permission-setups.html

***** Kinto setup 
Using docker image:
#!/bin/sh
docker run --rm --name  kinto --env-file ./kinto.env -p 8888:8888 kinto/kinto-server

$ cat kinto.env 
KINTO_STORAGE_BACKEND=kinto.core.storage.postgresql
KINTO_STORAGE_URL=postgres://postgres:postgres@10.0.2.99/kinto
MULTIAUTH_POLICIES=basicauth
KINTO_EXPERIMENTAL_PERMISSIONS_ENDPOINT=true
KINTO_PERMISSION_BACKEND=kinto.core.permission.postgresql
KINTO_PERMISSION_URL=postgres://postgres:postgres@10.0.2.99/kinto

***** Kinto schema - export doc 
Trying to generate a document will have the multi-commit issue as before:

#+BEGIN_SRC shell :results output
#echo '{"data": {"description": "emacs", "status": "TODO", "title": "emacs", "content": "some crazy content 2", "level": 2, "priority": null, "state": "", "properties": [], "completed": false }}' | http POST http://localhost:8888/v1/buckets/default/collections/newdoc/records --auth user1:pass1
#http GET http://localhost:8888/v1/buckets/default/collections/newdoc/records --auth something:something
echo '{"data": {"description": "emacs", "status": "TODO", "title": "emacs", "content": "some crazy content 2", "level": 2, "priority": null, "state": "", "properties": [], "completed": false }}' | http POST http://localhost:8888/v1/buckets/default/collections/newdoc/records --auth something:something

#+END_SRC

#+RESULTS:
: {"data":[{"properties":[],"id":"80c6d50c-7489-4493-9d32-2e513e0874ed","priority":null,"status":"TODO","level":2,"content":"some crazy content 2","completed":false,"title":"emacs","state":"","description":"emacs","last_modified":1504404010451},{"id":"2f0159f4-af8d-4950-b118-0c013b4a90d7","description":"emacs","last_modified":1504403945540}]}

#+BEGIN_SRC shell :results output
echo '{"data": {"description": "emacs", "status": "TODO", "title": "emacs", "content": "some crazy content 2", "level": 2, "priority": null, "state": "", "properties": [], "completed": false }}' | 
    http POST http://localhost:8888/v1/buckets/default/collections/tasks/records --auth user2:pass2
#+END_SRC

#+RESULTS:
: {"permissions":{"write":["basicauth:344805ac2a747906f79c7b1246cc1a5f15c1ebaffed6ca0bba111c3ec9defebf"]},"data":{"id":"c9cdc6e4-71a9-4756-85b1-365c2688526f","completed":false,"title":"emacs","level":2,"priority":null,"status":"TODO","state":"","content":"some crazy content 2","description":"emacs","properties":[],"last_modified":1504299805753}}

#+BEGIN_SRC shell :results output
    #http GET http://localhost:8888/v1/buckets/default/collections/tasks --auth user3:pass3
    http GET http://localhost:8888/v1/buckets/default/collections --auth user1:pass1
#+END_SRC

#+RESULTS:
: {"data":[{"id":"newdoc","last_modified":1504300871014},{"id":"tasks","order":["7ca72a3e-afe7-452e-a2ba-2dfa02a3d01c","0d37d5f1-4d44-443b-a546-52c6e15d8f0c"],"last_modified":1504135779282}]}

#+BEGIN_SRC shell :results output
    http GET http://localhost:8888/v1/buckets/default/collections/tasks/records --auth user1:pass1
#+END_SRC

#+RESULTS:
: {"data":[{"id":"981f166f-f033-4826-93eb-b5c3dc385815","title":"emacs","priority":null,"level":2,"status":"TODO","description":"emacs","state":"","properties":[],"content":"some crazy content 2","last_modified":1504138324556},{"id":"4e9dc027-45d1-4c48-8fe7-54d0af328d68","title":"emacs","priority":null,"level":1,"status":"TODO","description":"emacs","state":"","properties":[],"content":"some crazy content","last_modified":1504138145187},{"id":"53f9cdd5-fcd3-4699-924b-4d68067b25df","last_modified":1504138066386},{"id":"afea9854-508b-42a5-b490-0c23eed2d42b","level":1,"status":"todo","description":"user1 data try4","last_updated":"2017-08-30T08:30:00","last_modified":1504138038761},{"id":"9b6d746d-2e92-444c-93cb-56ebf28354d2","last_modified":1504138024204},{"id":"8bad1995-1585-422d-a712-eb7bc8a011b7","level":1,"status":"todo","description":"user1 data try3","last_updated":"2017-08-30T08:30:00","last_modified":1504137990660},{"id":"0348491e-a1dc-4470-9f0a-97bd6d17f338","last_modified":1504137897503},{"id":"aa867294-471b-4f78-bc1a-d5de45c3716e","last_modified":1504137845531},{"id":"a8e4c8fa-6315-4a56-b52e-39363bc1eba9","last_modified":1504137742919},{"id":"7e21c027-e2e3-4e09-806c-8846f9635a60","last_modified":1504137734302},{"id":"a11c5585-36f4-438e-979a-4b96111c81ff","last_modified":1504137676441},{"id":"0d37d5f1-4d44-443b-a546-52c6e15d8f0c","level":1,"status":"todo","description":"user1 data try2","last_updated":"2017-08-30T08:30:00","last_modified":1504135413068},{"id":"7ca72a3e-afe7-452e-a2ba-2dfa02a3d01c","level":1,"status":"todo","description":"user1 data","last_updated":"2017-08-30T08:30:00","last_modified":1504131793546}]}

#+BEGIN_SRC shell :results output
#    http GET http://localhost:8888/v1/buckets --auth user3:pass3 # 6f0b5857-7aaa-da44-f0bb-648c6619f5f1 is the bucket for user3:pass3
#    http GET http://localhost:8888/v1/buckets --auth user2:pass2 #  f1abf618-c58c-1496-5274-b7e8be8a259b is the bucket for user3:pass3
#     echo '{"permissions": {"read": ["basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39"]}}' | http PATCH http://localhost:8888/v1/buckets/default/collections/tasks/records/90dbf30d-1d5e-4874-8878-8ffcc67e8ffb -v --auth 'user2:pass2' # grant perms from user2 to user3 for record
  
    #http GET http://localhost:8888/v1/buckets/default/collections/tasks/records --auth user2:pass2
   # http GET http://localhost:8888/v1/buckets/default/collections/tasks/records --auth user3:pass3
#+END_SRC

#+RESULTS:
: {"data":[{"id":"683ea5c7-645b-4236-9f2f-48ec3b1542d9","completed":false,"title":"new item","last_modified":1504297795367},{"id":"8ad85f69-5056-47dc-9fdb-6fb473fc67e3","completed":false,"title":"New again","last_modified":1504286396202},{"id":"09c5b364-f358-41ff-b7f3-2fcb9470511a","completed":false,"title":"what","last_modified":1504273413555}]}

It worked, the perms table has:
 /buckets/6f0b5857-7aaa-da44-f0bb-648c6619f5f1                                                                | write      | basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39
 /buckets/6f0b5857-7aaa-da44-f0bb-648c6619f5f1/collections/tasks                                              | write      | basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39
 /buckets/6f0b5857-7aaa-da44-f0bb-648c6619f5f1/collections/tasks/records/09c5b364-f358-41ff-b7f3-2fcb9470511a | write      | basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39
 /buckets/6f0b5857-7aaa-da44-f0bb-648c6619f5f1/collections/tasks/records/8ad85f69-5056-47dc-9fdb-6fb473fc67e3 | write      | basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39
 /buckets/6f0b5857-7aaa-da44-f0bb-648c6619f5f1/collections/tasks/records/683ea5c7-645b-4236-9f2f-48ec3b1542d9 | write      | basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39
 /buckets/f1abf618-c58c-1496-5274-b7e8be8a259b/collections/tasks/records/90dbf30d-1d5e-4874-8878-8ffcc67e8ffb | read       | basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39

#+BEGIN_SRC shell :results output
#    http GET http://localhost:8888/v1/buckets/default/collections/tasks/records --auth user3:pass3
    http GET http://localhost:8888/v1/buckets/f1abf618-c58c-1496-5274-b7e8be8a259b/collections/tasks/records --auth user3:pass3
#+END_SRC

#+RESULTS:
: {"data":[{"id":"c9cdc6e4-71a9-4756-85b1-365c2688526f","completed":false,"title":"emacs","priority":null,"level":2,"status":"TODO","description":"emacs","state":"","properties":[],"content":"some crazy content 2","last_modified":1504299805753},{"id":"90dbf30d-1d5e-4874-8878-8ffcc67e8ffb","level":1,"status":"todo","description":"test1","last_updated":"2017-08-30T08:30:00","last_modified":1504298161138}]}

Trying to set perms on the collection tasks
#+BEGIN_SRC shell :results output
     echo '{"permissions": {"read": ["basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39"]}}' | http PATCH http://localhost:8888/v1/buckets/default/collections/tasks -v --auth 'user2:pass2' # grant perms from user2 to user3 for collection
  
    #http GET http://localhost:8888/v1/buckets/default/collections/tasks/records --auth user2:pass2
   # http GET http://localhost:8888/v1/buckets/default/collections/tasks/records --auth user3:pass3
#+END_SRC

#+RESULTS:
#+begin_example
PATCH /v1/buckets/default/collections/tasks HTTP/1.1
Content-Length: 106
Accept-Encoding: gzip, deflate
Host: localhost:8888
Accept: application/json
User-Agent: HTTPie/0.9.2
Connection: keep-alive
Content-Type: application/json
Authorization: Basic dXNlcjI6cGFzczI=

{"permissions": {"read": ["basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39"]}}


HTTP/1.1 200 OK
Access-Control-Expose-Headers: Retry-After, Alert, Backoff, Content-Length
Content-Length: 242
Content-Type: application/json
Date: Fri, 01 Sep 2017 21:05:06 GMT
Etag: "1504299906940"
Last-Modified: Fri, 01 Sep 2017 21:05:06 GMT
Server: waitress
X-Content-Type-Options: nosniff

{"permissions":{"write":["basicauth:344805ac2a747906f79c7b1246cc1a5f15c1ebaffed6ca0bba111c3ec9defebf"],"read":["basicauth:7fc2f542fc5a66bcbcb45bea54ec43c5db5020a03286ebb84dbb120ef66b2a39"]},"data":{"id":"tasks","last_modified":1504299906940}}
#+end_example

***** Kinto schema - How to generate the agendas
http://docs.kinto-storage.org/en/stable/api/1.x/filtering.html

#+BEGIN_SRC shell :results output
    http GET "http://localhost:8888/v1/buckets/default/collections/tasks/records?gt_level=1&status=TODO" --auth user1:pass1
#+END_SRC

#+RESULTS:
: {"data":[{"status":"TODO","state":"","id":"981f166f-f033-4826-93eb-b5c3dc385815","priority":null,"properties":[],"description":"emacs","level":2,"title":"emacs","content":"some crazy content 2","last_modified":1504138324556}]}

***** Kinto permissions
Synchronization happens at a bucket level. Each user has a default bucket which they sync automatically. Everything after that for 
  - tasks = new Kinto(remotedb).collection("tasks"), tasks.sync(syncOptions)
Separately need to fetch from different buckets. 

Operates on the bucket + records level . 
Can set sharing permission at the bucket, collection, or record level. Fetching anything of interest, however, is a completely different problem.

**** Todo app examples with Kinto
file:///home/tjheeta/repos/self/all-todo-test/kinto.js/demo/index.html
https://github.com/Kinto/kinto/wiki/App-examples
http://leplatrem.github.io/kinto-demo-calendar/#public
https://github.com/leplatrem/kinto-demo-calendar/
https://github.com/leplatrem/Routina

**** Frontend thoughts 
JS frontend is insane.

https://github.com/facebook/react  
https://github.com/developit/preact
https://github.com/angular/angular - 27000 - Too complicated.
https://github.com/meteor/meteor
https://github.com/emberjs/ember.js/
https://github.com/apollographql/apollo-client
https://github.com/vuejs/vue
https://github.com/MithrilJS/mithril.js - 
https://github.com/knockout/knockout - 8400 - MVVM
https://github.com/sveltejs/svelte - 5100 - compile time
https://github.com/turbolinks/turbolinks - 4000
https://github.com/unpoly/unpoly - 139
https://github.com/Polymer/polymer - 18000 - No idea what web components are.
https://github.com/zeit/next.js/ - 16700 - Server rendered react? Already have backend.
https://cycle.js.org/ - 


**** DONE Learning hyperapp to build a PWA/SPA
     CLOSED: [2017-09-01 Fri 10:16]
https://github.com/ramda/ramda - hyperapp probably needs something like this
https://github.com/lodash/lodash - or this
https://github.com/hyperapp/hyperapp/issues/244#issuecomment-321145114
https://github.com/hyperapp/hyperapp/blob/master/docs/vdom-events.md - adapting an external library
https://github.com/hyperapp/hyperapp/issues/251 - testing
***** DONE Need to figure out how to route to the next page - almost done
      CLOSED: [2017-08-31 Thu 01:11]
***** DONE Try to add kinto - mixin necessary?
      CLOSED: [2017-08-31 Thu 23:37]
** MVP2
Last state:
With great regret, moved phx to kinto. Massive refactor needed to not expose the data layer, copy and paste everywhere to get things working. Performance gone to hell also, for the hopes that the kinto sync is robust. Regardless, it will only need a rewrite between 100-1000 concurrent users depending on how expensive the sync is and that means it will need 5x-10x actual users. All to avoid having to write a sync library and figure out indexeddb. 

This stage looking to sync from server to local disk cleanly and get a functional agenda. 

*** DONE Thoughts
    CLOSED: [2017-09-01 Fri 17:47]
Need to clearly dilineate what happens server-side vs client-side. If items are being collaborated on, could open a websocket and make a lock for it. 
Comments - are they a separate system or do we write into the same schema?
Offline - editing is based on whether they are personal documents or not. Need a lock for shared. If go offline, warn that may have to merge conflicts or save duplicate version, but can continue editing.
Binpacking - does the client do it or the server?
If a person is involved in 50 objects, some public, delegated, some private. 
How do comments work? Client can connect to the backend system for comments, but what is real-time, and what is cached?  

Kinto requires bucket + collection level for anything. Specific buckets for fetching. In fact, if a document is a collection, then already have problems creating an agenda. Need to fetch all records in multiple collections, store them, build an agenda.  

Will be multiple calls to fetch unless we overlay our own queries into the database or maybe write a plugin.
  - https://github.com/Kinto/kinto-changes
This, however, won't help with changes. 

- Sync multiple collections. That sucks.
- Make a document a record. Then will have to parse and create the agenda each time. That sucks.
- Have tasks be replicated into an inbox bucket for a user. This doesn't really work except for maybe 2-way.  
- Build a custom offline cache and replication. Need to replicate everything that a user has access to, not just by bucket or collection. That sucks.
- Instead of each document being a collection, have a document collection and an object collection. Get a full list of objects to build an agenda. Ignore anything about a document. When sharing, can move the object into a shared bucket location and can still have reference to it? No, will have build problem again. 
- A dedicated group for collaboration with org-mode could have a bucket with group perms. After that, each can take objects offline, work, change state, etc.
- Move permissions directly onto the object itself.
- All objects for all users are in a single collection (could be sharded).
- Team-org-mode. Team/group has a bucket with two collections called documents and objects. Have a personal bucket. For each bucket, dosync. Personal bucket has agenda - which has metadata about the real object - title + location. Actually, can just build the agenda off of this without the sync. An object needs an "assigned" field also.
- Product usage. No one is going to partially share a document. The atomic unit for collaboration is the document, not the object. If anything, an object and children should be able to be moved to another document, with a reference back if necessary. Kinto does sharding by buckets => can represent a group. No one would share a subtree and get comments.
- Are comments objects? For instance, a collection of these objects can also represent a forum thread. A comment could be represented by a headline. Could put comments into their own collection? Maybe want to turn comments into objects. 
  
*** DONE Conclusion
    CLOSED: [2017-09-01 Fri 18:01]
1) Single user productivity 
   - agenda
   - notifications
   - capture
   - dumb server, smart client, bin-packing, etc. done in js
2) Collaboration -  Kinto can go pretty far with team/personal buckets with collections of doc + objects.
   - Each group is represented by a bucket. Can be sharded. 
   - Can build agenda - need to resolve over all buckets.
   - Can share documents / move documents / create documents from object subtrees if necessary.
   - Locking for editing documents online via websocket
   - Can shared and assign tasks by fetch from multiple buckets/collections.
   - External contributors also work by adding to a single group. All hierarchy needs to be done via bucket-id or secondary table.
   - Moving groups from public to private - offline stuff would still exist, but could remove keys from keychain?
   - Syncing multiple buckets for sharing allows interesting E2E encryption possibilities.
   - Search - do we do it in the browser if everything is encrypted - depends on transform on sync + indexeddb performance

     
*** DONE Braindump - Elixir to Kinto
    CLOSED: [2017-09-02 Sat 17:59]
    :LOGBOOK:
    CLOCK: [2017-09-01 Sat 22:30]--[2017-09-02 Sat 00:27] =>  1:57
    CLOCK: [2017-09-01 Fri 20:24]--[2017-09-01 Fri 20:49] =>  0:25
    :END:

Logged into a session:
http://localhost:4000/sessions/new
http://localhost:4000/sessions/ef73a51a0770908c273ba7f20c6b6c308225c708d76981f18cf3d07a5f98

Need some sort of token to be able to write to kinto appropriately.
Stick with basicauth and use userid:sometoken to authorize against kinto. That's as static as can get. Can't use email since they might want to change it. Kinto auths with user:password for basic auth.  
Not in the mood to implement 3-legged auth with bearer tokens:
http://docs.kinto-storage.org/en/stable/tutorials/authentication-github.html
"The user id github:<username> can now be used in permissions definitions. It is much more convenient than Basic Auth identifiers!"

So no direct access to kinto from the client without having a mechanism to fetch the userid from elixir server.
4-step auth - Somehow client authorizes against kinto 3-legged, kinto looks up the uuid from server.

Experimental accounts api:
http://docs.kinto-storage.org/en/stable/api/1.x/accounts.html

At what point is it faster to write offline replication handler for indexeddb than muck with kinto?
Or just write into kinto schema directly with ecto? 
kinto.js replication needs a user/password. Login normally, use user_id as token which can be used on sync. If not proxied, can steal all the data from kinto directly on losing the user token or kinto token. The whole point of nopassword was not to be having these secrets. 

- Need the server and the client to both be able to write into kinto.
- Another option is the client also does the parsing and there is no server at all.

https://github.com/standardnotes/web/blob/master/app/assets/javascripts/app/services/syncManager.js

*** DONE Elixir to Kinto
    CLOSED: [2017-09-02 Sat 17:36]
    :LOGBOOK:
    CLOCK: [2017-09-02 Sat 15:00]--[2017-09-02 Sat 17:30] =>  2:30
    CLOCK: [2017-09-02 Sat 13:20]--[2017-09-02 Sat 14:23] =>  1:03
    CLOCK: [2017-09-02 Sat 12:00]--[2017-09-02 Sat 13:10] =>  1:10
    CLOCK: [2017-09-02 Sat 11:17]--[2017-09-02 Sat 11:47] =>  0:30
    CLOCK: [2017-09-02 Sat 10:30]--[2017-09-02 Sat 11:16] =>  0:46
    CLOCK: [2017-09-02 Sat 10:05]--[2017-09-02 Sat 10:30] =>  0:25
    CLOCK: [2017-09-02 Sat 09:35]--[2017-09-02 Sat 10:00] =>  0:25
    CLOCK: [2017-09-02 Sat 09:05]--[2017-09-02 Sat 09:30] =>  0:25
    :END:

#+BEGIN_SRC shell :results output
    http GET "http://localhost:8888/v1/buckets/default/collections/documents/records/someid" --auth kinto_token:de4927d9-a099-4ec8-bb99-4f69888acb34
#+END_SRC

#+RESULTS:
: {"errno":110,"code":404,"details":{"id":"someid","resource_name":"record"},"error":"Not Found"}


*** DONE Set constant user_hmac_secret
    CLOSED: [2017-09-02 Sat 18:00]
$ cat kinto.env 
KINTO_STORAGE_BACKEND=kinto.core.storage.postgresql
KINTO_STORAGE_URL=postgres://postgres:postgres@10.0.2.99/kinto
MULTIAUTH_POLICIES=basicauth
KINTO_EXPERIMENTAL_PERMISSIONS_ENDPOINT=true
KINTO_PERMISSION_BACKEND=kinto.core.permission.postgresql
KINTO_PERMISSION_URL=postgres://postgres:postgres@10.0.2.99/kinto
KINTO_USERID_HMAC_SECRET=not_really_a_secret
KINTO_BATCH_MAX_REQUESTS=10000

*** DONE Security model for kinto and replication
    CLOSED: [2017-09-03 Sun 01:17]
- Access to kinto only through phx.
  - if not, need to verify 3rd party github:<username> as identifier for kinto auth + Repeat for phx auth 
  - phx directly to postgres 
- Must be authenticated for api access.
- User forgery prevented by phx signed session cookies.
- Rewrites the header from the client and enforces the authorization header to be only which is the session cookie.

Other pros - abstracting out the data layer, could be replaced later.

Server-writes - if write directly from phx to postgres in kinto schema, need to know what buckets or need to implement custom uuid generation. 
**** DONE Setup a path for a proxy and rewrite any request headers with basic auth info
     CLOSED: [2017-09-02 Sat 21:46]
     :LOGBOOK:
     CLOCK: [2017-09-02 Sat 18:00]--[2017-09-02 Sat 21:46] =>  3:46
     :END:
**** DONE Change client url
     CLOSED: [2017-09-03 Sun 01:17]
     :LOGBOOK:
     CLOCK: [2017-09-02 Sun 22:21]--[2017-09-03 Sun 01:21] =>  3:00
     :END:
Missed a spot. Kinto.js cannot send a request out without knowing the cookie. The cookie is set to http-only. Ergo - kinto cannot access cookie, ergo - it is really secure.  
"<html> <head> <script> fetch('/bla, { credentials: 'same-origin' }) </script> </head> </html>" works.

https://developer.mozilla.org/en-US/docs/Web/API/Request/mode
https://stackoverflow.com/questions/30013131/how-do-i-use-window-fetch-with-httponly-cookies-or-basic-auth
https://github.com/github/fetch
kinto-http.js has a bug that it doesn't send the credentials.

*** DONE Authorization
    CLOSED: [2017-09-03 Sun 01:25]
**** DONE User security investigation
     CLOSED: [2017-09-01 Fri 21:30]
     :LOGBOOK:
     CLOCK: [2017-09-01 Fri 20:50]--[2017-09-01 Fri 21:30] =>  0:40
     :END:
- Phoenix by default signs session cookie with secret_key_base
- Can also encrypt the cookie completely
- https://elixirforum.com/t/how-is-phoenix-token-different-from-jwt/2349/5
- https://elixirforum.com/t/phoenix-token-encryption/3123
**** DONE Kinto proxying
     CLOSED: [2017-09-03 Sun 01:25]

*** Patch kinto-http.js

line 164 of src/http.js - setting default credentials to same-origin should fix it
  async request(url, request = { headers: {} }, options = { retry: 0 }, credentials = 'same-origin') {
    // Ensure default request headers are always set                            
    request.headers = { ...HTTP.DEFAULT_REQUEST_HEADERS, ...request.headers };  
    request.credentials = credentials;          

*** DONE Add REST API for sync to/from emacs
    CLOSED: [2017-09-03 Sun 21:14]
Doing an api endpoint only that is for curl / api token. Client view for upload comes later and probably in PWA. [[Upload UI]] 
Client will periodically need to sync documents and objects collections.
**** DONE Add api key to plug
      CLOSED: [2017-09-03 Sun 10:38]
      :LOGBOOK:
      CLOCK: [2017-09-03 Sun 10:00]--[2017-09-03 Sun 10:38] => 0:38
      :END:

#+BEGIN_SRC shell :results output
#    curl -H "x-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34" -H "x-api-key:fail" "http://localhost:4000/api/v1" 
    curl -H "x-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34" -H "x-api-key:somekey" "http://localhost:4000/api/v1/documents" 
#+END_SRC

#+RESULTS:
: {"data":[{"name":"/tmp/plug-1504/multipart-1504474364-875480956313730-1","id":"bad5ed41-e053-4f57-a66b-d30df6edce35"},{"name":"/tmp/plug-1504/multipart-1504474293-102208735700548-2","id":"0072d402-55f4-4e83-8657-b8e09f73f25f"},{"name":"/tmp/plug-1504/multipart-1504474253-149974857867695-1","id":"d7d1eaf7-d106-49b6-a6f3-42d0f908f0b6"},{"name":"/tmp/plug-1504/multipart-1504474221-746897423043180-2","id":"50f98993-8eec-4541-ae72-dc32c28bc56a"},{"name":"/tmp/plug-1504/multipart-1504473279-701812867137179-2","id":"256b079d-b56c-4b39-aad5-36efec30a910"}]}


**** DONE fix that api + session should get same results
      CLOSED: [2017-09-03 Sun 14:42]
      :LOGBOOK:
      CLOCK: [2017-09-03 Sun 11:00]--[2017-09-03 Sun 14:42] => 3:42
      :END:
#+BEGIN_SRC shell :results output
#    curl -H "x-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34" -H "x-api-key:fail" "http://localhost:4000/api/v1" 

# access to kinto is only through session cookie, not api_key?
# curl -X POST -d "test" http://localhost:4000/v1/buckets/default/collections/documents/records -H 'Cookie: _ttl_key=SFMyNTY.g3QAAAADbQAAAAtfY3NyZl90b2tlbm0AAAAYcXZ4VEdicHB0QXNJK0VBaEJreFNFUT09bQAAAAxjdXJyZW50X3VzZXJtAAAAF3Rlc3R1c2VyQGZ4bWFuaWZvbGQuY29tbQAAAAd1c2VyX2lkbQAAACRkZTQ5MjdkOS1hMDk5LTRlYzgtYmI5OS00ZjY5ODg4YWNiMzQ._OzavlEQsF4VR-N9iU-Poj2fX3whC1FpVvpxSsXue48'
#    echo '{"data": {"name": "tmpfile.org" }}' | http POST http://localhost:8888/v1/buckets/default/collections/documents/records --auth user1:pass1

#    curl -H "x-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34" -H "x-api-key:somekey" "http://localhost:4000/api/v1/documents" 
#    | http "x-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34" "x-api-key:somekey" "http://localhost:4000/api/v1/documents" 
    http GET http://10.0.0.175:4000/v1/buckets/default/collections/tasks/records X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
#+END_SRC

The api auth and the session auth now work and give same access.
Need to be able to upload a file to the api endpoint and have it imported

**** DONE import endpoint (create) 
      CLOSED: [2017-09-03 Sun 21:09]
#+BEGIN_SRC shell :results output
  #    http GET http://10.0.0.175:4000/v1/buckets/default/collections/documents/records X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
   #   http GET http://10.0.0.175:4000/api/v1/documents X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
  http --form POST http://10.0.0.175:4000/api/v1/documents X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey file@README.org
#+END_SRC
#+RESULTS:
: {"data":[{"name":"/tmp/plug-1504/multipart-1504498094-716839023744787-2","id":"9dafb409-ab95-43ea-a774-ae31d361be5f"}]}
**** DONE Export endpoint (show)
      CLOSED: [2017-09-03 Sun 21:09]

#+BEGIN_SRC shell :results output
#    http GET http://10.0.0.175:4000/v1/buckets/default/collections/documents/records X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
 #   http --form POST http://10.0.0.175:4000/api/v1/documents X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey file@README.org
#    http GET http://10.0.0.175:4000/api/v1/documents X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
    http GET http://10.0.0.175:4000/api/v1/documents/9dafb409-ab95-43ea-a774-ae31d361be5f X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
#+END_SRC

**** DONE delete document (delete)
      CLOSED: [2017-09-03 Sun 21:09]
      :LOGBOOK:
      CLOCK: [2017-09-03 Sun 20:15]--[2017-09-03 Sun 21:09] =>  0:54
      :END:
#+BEGIN_SRC shell :results output
#    http GET http://10.0.0.175:4000/v1/buckets/default/collections/documents/records X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
 #   http --form POST http://10.0.0.175:4000/api/v1/documents X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey file@README.org
#    http GET http://10.0.0.175:4000/api/v1/documents X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
    #http DELETE http://10.0.0.175:4000/api/v1/documents/0072d402-55f4-4e83-8657-b8e09f73f25f X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
    #http DELETE http://10.0.0.175:4000/api/v1/documents/d7d1eaf7-d106-49b6-a6f3-42d0f908f0b6 X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
    #http DELETE http://10.0.0.175:4000/api/v1/documents/50f98993-8eec-4541-ae72-dc32c28bc56a X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
    http DELETE http://10.0.0.175:4000/api/v1/documents/256b079d-b56c-4b39-aad5-36efec30a910 X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
#+END_SRC

#+RESULTS:

**** DONE User security
     CLOSED: [2017-09-03 Sun 09:03]

**** DONE Need api_token field - forget having a separate table for now
     CLOSED: [2017-09-03 Sun 14:39]
*** DONE Import/Export Bugs
    CLOSED: [2017-09-04 Mon 01:07]
**** DONE scheduled_date_range, scheduled_time_interval not written to db
     CLOSED: [2017-08-19 Sat 22:43]
**** DONE Regenerate org-file - scheduled date - the second date of the range is missing
     CLOSED: [2017-09-04 Mon 01:07]
     :LOGBOOK:
     CLOCK: [2017-09-03 Sun 23:06]--[2017-09-04 Mon 01:07] =>  2:01
     :END:
[[file:~/repos/self/ttl/lib/ttl/parse/export.ex::defp%20db_date_to_string(date,%20bracket,%20time_interval,%20date_range,%20repeat_interval%20)%20do][fix it here]]
*** Sync - export with ids
*** Sync REST client - python?
   http --form POST http://10.0.0.175:4000/api/v1/documents X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey file@README.org

1) Start off with a simple script to do whole files
2) Show api needs options to add id
3) Backups of files synced

*** How to move between SPA and PHX? 
Or remove all html on PHX? Just have PHX be a pure API. 
*** Cleanup backlog
**** Make sessions last 10 years, not expire on logout
**** state may get populated incorrectly if the first word is caps.
Doesn't affect export or headline though...it's a bug that is not quite a bug
Essentially, searches for those titles may break .
**** The document collection should also have a userid in upload
Right now kinto doesn't have this concept, just maps to buckets for ownership.
**** name metadata isn't right on import
**** Go through each query and make sure error conditions are satisfied.
[[file:lib/ttl/web/controllers/api_document_controller.ex::def%20delete(conn,%20%25{"id"%20=>%20id})%20do][example]]
**** Go through things.ex and normalize structures/responses?
**** Fallback controller
[[file:lib/ttl/web/controllers/api_document_controller.ex::#%20TODO%20-%20fallback][fallback location]]
[[file:lib/ttl/web/controllers/fallback_controller.ex::#%20TODO%20not%20being%20used%20yet][fallback controller]]
**** kinto token == user id
Are we going to have a separate kinto_token? Will the token be available to clients ever?
[[file:lib/ttl/accounts/plug_auth.ex::#%20TODO%20-%20kinto%20token%20==%20user_id][cleanup in auth]]
**** Tests for authorization
**** Figure out how to put the SPA on phx
**** DONE Why are we treating api auth different from session auth? 
     CLOSED: [2017-09-03 Sun 14:12]
Shouldn't it check if api key
If not, check session key
Merge the two together and give access to everything if moving to SPA.
Regarding kinto, people can put arbitrary data in local indexeddb anyway. 
Kinto token needs to be fixed, that is userid/client_id. If they get api_key or session cookie, they have access. 
**** Refactor data layer and remove copy and paste in import/exports
**** Setup a http pool for kinto
**** Move kinto to https
**** Hardcoded localhost
**** Postgres? Remove or refactor and keep for later?
**** Remove all the extra phx routes
**** Make an elixir-kinto adapter?
- new table has userid + kinto token
- connection pool
- return structs

What about logbook?
Need to revamp the schema. 
**** add spec to all functions
*** App - Pre and post syncing data immediately / retries / backoffs
*** App - How to have multiple pages/routes without going insane.
*** Pick and schedule things onto agenda :ms.agenda:
*** Secondary collection for tags and properties :ms.agenda:
     :LOGBOOK:
     CLOCK: [2017-09-03 Sun 21:50]--[2017-09-03 Sun 22:15] =>  0:25
     :END:
Now that this is with kinto, will have to be per object - maybe store a secondary collection in a bucket with count
Considering this is in each object, will have to store as an array of str. Can kinto search on that?

Will also need to do this locally in indexeddb . 
https://www.codeproject.com/Articles/744986/How-to-do-some-magic-with-indexedDB

Kinto doesn't seem to be able to do it.
1) subarray in object - doesn't appear to work
2) separated by :, strings in tag array - maybe do to rebuild the file, but doesn't look like the js adapter will do it out of box.
3) separate tags collection - and dealing with referential integrity issues may be the best bet.
   - on import - store the tag as a string. guaranteed to be right. secondary tags collection for finding objects with tags.
[[*test via hash][test via hash]] seems to work for python api. Need to sync it locally and then test via js . Api doesn't look like it supports. 
***** DONE testing kinto
      CLOSED: [2017-09-03 Sun 22:58]

****** Testing via array
#+BEGIN_SRC shell :results output
#    http POST http://localhost:8888/v1/buckets/default/collections/tasks/records --auth user2:pass2

#echo '{"data": {"id": "tjtest2" , "description": "tjtest", "status": "TODO", "title": "tjtest", "properties": [], "tags": ["test", "test1", "test2"] }}' | 
#echo '{"data": {"id": "tjtest3" , "description": "tjtest", "status": "TODO", "title": "tjtest", "properties": [], "tags": ["test3", "tag1"] }}' | 
#echo '{"data": {"id": "tjtest4" , "description": "tjtest", "status": "TODO", "title": "tjtest", "properties": [], "tags": ["test4", "tag1"] }}' | 
#echo '{"data": {"id": "tjtest5" , "description": "tjtest", "status": "TODO", "title": "tjtest", "properties": [], "tags": "tag1" }}' | 
echo '{"data": {"id": "tjtest6" , "description": "tjtest", "status": "TODO", "title": "tjtest", "properties": [], "tags": ["tag1"] }}' | 
    http POST http://10.0.0.175:4000/v1/buckets/default/collections/objects/records  X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey

#    http GET "http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?in_tags=tag1" X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?tags=[test]
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?tags=[]
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?tags=[tags1,test3]
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?tags=tag1 - 1 record
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?tags=[tag1] - 0 records
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?in_tags=tag1,test3 - 1 record
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?tags=[tag1] - should be 1 record
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?tags=["tag1"] - 1 record
#+END_SRC

****** test via hash
#+BEGIN_SRC shell :results output
#echo '{"data": {"id": "hash1" , "description": "tjtest", "status": "TODO", "title": "tjtest", "properties": {"STYLE": "habit", "LAST_REPEAT":1234}, "tags": {"tag1":1, "tag3": 1} }}' | 
#echo '{"data": {"id": "hash2" , "description": "tjtest", "status": "TODO", "title": "tjtest", "properties": {"STYLE": "habit", "LAST_REPEAT":1234}, "tags": {"tag1":1, "tag2": 1} }}' | 
echo '{"data": {"id": "hash3" , "description": "tjtest", "status": "TODO", "title": "tjtest", "properties": {"STYLE": "habit2", "LAST_REPEAT":1234}, "tags": {"tagx": 1, "tag2": 1} }}' | 
    http POST http://10.0.0.175:4000/v1/buckets/default/collections/objects/records  X-api-client:de4927d9-a099-4ec8-bb99-4f69888acb34 X-api-key:somekey

#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?properties.STYLE=habit
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?tags.tag1=1
#http://10.0.0.175:4000/v1/buckets/default/collections/objects/records?tags.tag2=1
#+END_SRC

#+RESULTS:
: {"permissions":{"write":["basicauth:09e0334b23e025416bf1836378ad07e2d14488137383f8232ac74987fc1ce4e6"]},"data":{"properties":{"STYLE":"habit2","LAST_REPEAT":1234},"id":"hash3","tags":{"tag2":1,"tagx":1},"status":"TODO","title":"tjtest","description":"tjtest","last_modified":1504513377299}}


**** More complete tests for generate -> regenerate with UUID's

*** Sync testing and performance
Upload file -> import -> conflict items -> export -> return {new_file, conflicts, updated, published, etc.)
      - Need to modify the object from ui/db and then test uploads
      - make sure the item that is being updated has same version => last-modify time in unixtime utc. On sync, replace with max(device_time, server_time)
      - Can't use version number. Concurrency - multiple people could have modified the item. Clocks not good enough.
      - Version could be a JSON CRDT to enable merging, but need to make emacs module also
**** Recursive sync testing
Import file
Export file with new ids
Import -> same amount of entries in database
Make change to file - modify vs add new nodes without ids
Export -> same result
*** Offline test
*** Notifications
- If these end up having to be via the server, perhaps bin-packing / re-scheduling also on the server.
*** App - Display agenda :ms.agenda:
**** UI Display
**** Onclick and onslide events - increment version
**** Onedit - increment version
*** App - Components
https://github.com/hyperapp/hyperapp/issues/238#issuecomment-310999839
*** App - How to include different files
*** Need to namespace buckets based on org/groupname/id? :ms.shared:

*** Bin-packing
*** User settings
*** Daily review
*** Weekly review
*** Connect to phoenix channels - why? Comments/feedback?
*** Check for updates of code versions and update notice
*** notifications
*** App - UI Tests
https://github.com/webpro/Automated-SPA-Testing
*** Import/export Tests
       Updates
       - An old object (all db counts same)
       - A new object gets added (db count + 1)
       - the new object gets modified (db count same)
** MVPX
*** Add an editor?
http://scripting.com/2013/09/16/concordOurGplOutliner
https://github.com/tjheeta/concord

*** Attachments and inline viewing
**** Inline viewing
[[file:~/org/jive/main/mako/deploy_flow.png][file:~/org/jive/main/mako/deploy_flow.png]]
:org-toggle-inline-images or c-c, c-x, c-v 

**** Attachments

*** Server-side rendering - how does this even matter?
*** capture
**** bookmarklets directly to server
**** attachments                                                     :ATTACH:
     :PROPERTIES:
     :Attachments: deploy_flow.png developer_access.png
     :ID:       002c1bf3-a3cb-45a2-856a-93f7fbb3e6a0
     :END:
- The directory is actually in ttl/data (based on where this org file is
- http://orgmode.org/manual/Attachments.html
**** pictures and attachments
     :PROPERTIES:
     :ID:       9e0c00b2-4301-4b75-bf4e-a11f4ab31fcb
     :END:
https://stackoverflow.com/questions/17435995/paste-an-image-on-clipboard-to-emacs-org-mode-file-without-saving-it
**** some sort of ifttt integration? send email to an interface and it will go into todo?
*** parse additional properties for scheduling + contexts + tags
estimated time
*** Inbox - integration point for externs and updates?
*** Add full calendar?
*** Google calendar / ICS
*** Conflict resolution?
- Show conflicts
- Pick versions
- Force upload
*** Capture to different documents
*** Set concurrency controls?
- Will need to pass on headers in KintoPlugProxy
*** Upload UI
Need a single interface that is accessible through PWA to import/export.
The organization of this is getting a bit messy. 
The client is based on being offline first and using kinto.
If the client wants to import a document, where is the view? 
To import:
1) User posts the file somewhere (api or endpoint)
   - Curl -> api token and result
   - PWA -> already have the session cookie, so have access to everything. 
     - Need a view for documents and have an upload button. 
     - state tree gets loaded from indexeddb, includes the document and all the objects
   - Phx -> Easy, but how would this get synced back to the client? Super-weird?
2) File gets parsed and written to kinto
3) Kinto synchronizes the data to the client for viewing
**** Document api needs to have multiple options
- force upload
- name
- if no file, just create a blank doc?
*** Autocomplete tags and documents
*** Production Authorization 
    :LOGBOOK:
    CLOCK: [2017-09-01 Fri 21:30]--[2017-09-01 Fri 22:48] =>  1:18
    :END:

https://github.com/sakurity/securelogin
http://inaka.net/blog/2016/07/27/passwordless-login-with-phoenix/
https://medium.com/the-many/how-we-built-passwordless-authentication-with-auth0-and-elixir-phoenix-ea508c9f3490
How can you check if a token is revoked without going to the db?
Have a global counter of revoke, token has a counter element, if it is less than counter, it needs re-auth. Very fragile under load.

**** Need api/bucket tokens for rest api
**** Per device tokens or just revoke all and re-create current?
**** How to revoke a user/device?
Refresh token in db that is verified on refresh - 30 days. Just store that many days of revoke tokens on each refresh token use.
Auth token regenerate every 15 min 
if don't use tokens and just use a session in ets/dets, still won't work properly with long-session needing to be in mem or db. keeping n-days of revocation tokens better than keeping all auth tokens.   
**** Ensure session cookie containing token is "HttpOnly" "Secure" (https)
**** Cookie expiry far into the future. 
**** Cookies are fully encrypted
**** CSRF protection
#    plug :protect_from_forgery
Right now are avoiding XSS attacks with http-only cookies and using credentials
CSRF is still possible. With the plug protect_from_forgery, it will write it into the session.
Need to modify one of the plugs to write it into the header request, if not already there. 

http://www.redotheweb.com/2015/11/09/api-security.html

   :plug_session => %{"_csrf_token" => "LLT2BMJp7DiKlDc/DQdsMw==",
     "current_user" => "a@b.com",
     "user_id" => "de4927d9-a099-4ec8-bb99-4f69888acb34"},

https://github.com/elixir-plug/plug/issues/504

**** websocket security
- read assets/js/socket.js
- https://hexdocs.pm/phoenix/Phoenix.Token.html
- https://medium.com/caspertechteam/securing-websockets-in-elixir-770cbd2da043
- http://blog.stratumsecurity.com/2016/06/13/websockets-auth/
- Send the token under bearer?

Each document in document collection
Each object is now a record in objects collection. 
Personal buckets are auto-synced.
*** Assistant - location based
*** Elisp client - update versions / conflict resolution, local changes :ms.faraway:
*** Encryption per bucket 
Need to have a master password. Each user has a keychain for all accessible buckets. The buckets get synced locally, decrypted as necessary, and stuff built. Re-keying will be massively painful. 
*** Security
https://turtlapp.com/
https://standardnotes.org/blog/7/announcing-our-2017-security-audit-results
https://github.com/standardfile
*** Security - Moving groups from public to private - offline stuff would still exist. 
*** CRDT editing :ms.shared:
- https://github.com/xwiki-contrib/chainpad
- https://hal.archives-ouvertes.fr/inria-00336191/
- https://news.ycombinator.com/item?id=12303100
- https://pages.lip6.fr/Marek.Zawirski/papers/SwiftCloud-RR-8347.pdf
* Scratch

#+BEGIN_SRC elixir :results output
2 + 2
#+END_SRC

* clocktable
#+BEGIN: clocktable :maxlevel 5 :scope file
#+CAPTION: Clock summary at [2017-09-04 Mon 01:07]
| Headline                                         | Time       |          |         |       |      |
|--------------------------------------------------+------------+----------+---------+-------+------|
| *Total time*                                     | *3d 14:56* |          |         |       |      |
|--------------------------------------------------+------------+----------+---------+-------+------|
| Ttl                                              | 3d 14:56   |          |         |       |      |
| \_  MVP1                                         |            | 2d 12:56 |         |       |      |
| \_    import/export to api.                      |            |          | 1d 4:50 |       |      |
| \_      consolidate sections                     |            |          |         |  2:07 |      |
| \_      Need to fix the planning parser to...    |            |          |         |  0:25 |      |
| \_      implement db schema                      |            |          |         |  1:15 |      |
| \_      write to database                        |            |          |         | 11:06 |      |
| \_        Make it into a function                |            |          |         |       | 0:30 |
| \_        Make a solid decision on what to do... |            |          |         |       | 1:03 |
| \_        Parse and cast the dates               |            |          |         |       | 4:58 |
| \_      Need to parse the file metadata          |            |          |         |  1:35 |      |
| \_      regenerate file from database - with...  |            |          |         |  4:59 |      |
| \_      Properties - parse, store, and write     |            |          |         |  1:47 |      |
| \_      Add version and id into regenerated...   |            |          |         |  0:24 |      |
| \_      Read file uuid from file to pull objects |            |          |         |  2:07 |      |
| \_      Start of file exception                  |            |          |         |  0:25 |      |
| \_      Ensure data gets updated on change       |            |          |         |  0:08 |      |
| \_      make an interval data structure for...   |            |          |         |  1:15 |      |
| \_      Parse the schedule properly into the...  |            |          |         |  1:17 |      |
| \_    Cleanup                                    |            |          |    3:19 |       |      |
| \_      Refactor parse                           |            |          |         |  3:19 |      |
| \_    POC - display UI for agenda                |            |          | 1d 4:47 |       |      |
| \_      Kinto                                    |            |          |         |  4:48 |      |
| \_  MVP2                                         |            | 1d 2:00  |         |       |      |
| \_    Braindump - Elixir to Kinto                |            |          |    2:22 |       |      |
| \_    Elixir to Kinto                            |            |          |    7:14 |       |      |
| \_    Security model for kinto and replication   |            |          |    6:46 |       |      |
| \_      Setup a path for a proxy and rewrite...  |            |          |         |  3:46 |      |
| \_      Change client url                        |            |          |         |  3:00 |      |
| \_    Authorization                              |            |          |    0:40 |       |      |
| \_      User security investigation              |            |          |         |  0:40 |      |
| \_    Add REST API for sync to/from emacs        |            |          |    5:14 |       |      |
| \_      Add api key to plug                      |            |          |         |  0:38 |      |
| \_      fix that api + session should get...     |            |          |         |  3:42 |      |
| \_      delete document (delete)                 |            |          |         |  0:54 |      |
| \_    Import/Export Bugs                         |            |          |    2:01 |       |      |
| \_      Regenerate org-file - scheduled date...  |            |          |         |  2:01 |      |
| \_    Secondary collection for tags              |            |          |    0:25 |       |      |
| \_    Production Authorization                   |            |          |    1:18 |       |      |
#+END:
